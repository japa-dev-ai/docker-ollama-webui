# Ollama AI com WebUI - Docker Setup

Este projeto configura um ambiente baseado em Docker para rodar o **Ollama AI** como um servidor de modelos de IA e uma **WebUI** para interaÃ§Ã£o via chat.

## ğŸ“Œ Requisitos

- Docker e Docker Compose instalados no sistema.
- ConexÃ£o com a internet para baixar os modelos e dependÃªncias.

## ğŸ“¦ Estrutura do Projeto

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ modelos/  # (Opcional: para armazenar modelos baixados)
â””â”€â”€ webui/
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ app/      # (Seu cÃ³digo WebUI)
```

## ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o

1. **Clone o repositÃ³rio:**

   ```sh
   git clone https://github.com/japa-dev-ai/docker-ollama-webui.git
   cd docker-ollama-webui
   ```

2. **Suba os containers:**

   ```sh
   docker-compose up -d
   ```

3. **Acesse os serviÃ§os:**

   - **Ollama API:** `http://localhost:11434`
   - **WebUI:** `http://localhost:3000`

## ğŸ“œ ConfiguraÃ§Ã£o do Docker

### Dockerfile do **Ollama Server** (`./ollama/Dockerfile`)

```dockerfile
# Usa uma imagem base com suporte a CUDA
FROM nvidia/cuda:12.2.2-base-ubuntu22.04

# Instala dependÃªncias
RUN apt-get update && apt-get install -y \
    wget \
    python3 \
    python3-pip \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.ai/install.sh | bash
RUN pip install ollama

# Define o diretÃ³rio de trabalho
WORKDIR /app

# Baixa o modelo deepseek-r1:7b
RUN ollama pull deepseek-r1:7b

# ExpÃµe a porta padrÃ£o do Ollama
EXPOSE 11434

# Comando para iniciar o Ollama
CMD ["ollama", "serve"]
```

### Dockerfile do **WebUI** (`./webui/Dockerfile`)

```dockerfile
# Usa uma imagem base Node.js
FROM node:18

# Define o diretÃ³rio de trabalho
WORKDIR /app

# Copia os arquivos do projeto
COPY . .

# Instala as dependÃªncias
RUN npm install

# ExpÃµe a porta padrÃ£o do OpenWebUI
EXPOSE 3000

# Comando para iniciar o OpenWebUI
CMD ["npm", "start"]
```

### `docker-compose.yml`

```yaml
version: '3.8'

services:
  ollama:
    build:
      context: .
      dockerfile: ./ollama/Dockerfile
    container_name: ollama
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all # Habilita todas as GPUs disponÃ­veis
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ollama_data:/app/data # Persiste os dados do Ollama
    networks:
      - ollama_network
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu] # Reserva recursos de GPU

  openwebui:
    build:
      context: .
      dockerfile: ./webui/Dockerfile
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:3000" # ExpÃµe a porta do OpenWebUI
    environment:
      - OLLAMA_API_URL=http://ollama:11434 # Conecta ao Ollama
    depends_on:
      - ollama
    networks:
      - ollama_network

volumes:
  ollama_data:

networks:
  ollama_network:
    driver: bridge
```

## ğŸ›  Comandos Ãšteis

### Parar e remover containers

```sh
docker-compose down
```

### Ver logs

```sh
docker-compose logs -f
```

### Acessar o container

```sh
docker exec -it nome_do_container bash
```

## ğŸ“ ContribuiÃ§Ã£o

Sinta-se Ã  vontade para abrir **issues** e **pull requests** para melhorar o projeto! ğŸš€

## ğŸ“„ LicenÃ§a

GNU General Public License v3.0
